{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 38,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      },
      "source": [
        "%%pyspark\n",
        "\n",
        "df = spark.read.load('abfss://capture@splacceler5lmevhdeon4ym.dfs.core.windows.net/SeattlePublicLibrary/Checkouts_by_Title.csv', format='csv'\n",
        "## If header exists uncomment line below\n",
        ", header=True\n",
        ")\n",
        "display(df.limit(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": true
      },
      "source": [
        "%%pyspark\r\n",
        "\r\n",
        "# Show Schema\r\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": true
      },
      "source": [
        "%%pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Primary storage info\n",
        "capture_account_name = 'splacceler5lmevhdeon4ym' # fill in your primary account name\n",
        "capture_container_name = 'capture' # fill in your container name\n",
        "capture_relative_path = 'SeattlePublicLibrary/Checkouts_by_Title.csv' # fill in your relative folder path\n",
        "\n",
        "capture_adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (capture_container_name, capture_account_name, capture_relative_path)\n",
        "print('Primary storage account path: ' + capture_adls_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      },
      "source": [
        "%%pyspark\r\n",
        "\r\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType, TimestampType\r\n",
        "csvSchema = StructType([\r\n",
        "  StructField('usage_class', StringType(), True),\r\n",
        "  StructField('checkout_type', StringType(), True),\r\n",
        "  StructField('material_type', StringType(), True), \r\n",
        "  StructField('checkout_year', IntegerType(), True),\r\n",
        "  StructField('checkout_month', IntegerType(), True),\r\n",
        "  StructField('checkouts', IntegerType(), True),   \r\n",
        "  StructField('title', StringType(), True),\r\n",
        "  StructField('creator', StringType(), True),\r\n",
        "  StructField('subjects', StringType(), True),\r\n",
        "  StructField('publisher', StringType(), True),\r\n",
        "  StructField('publication_year', StringType(), True)   \r\n",
        "])\r\n",
        "\r\n",
        "CheckByTPI_capture_df = spark.read.format('csv').option('header', 'True').schema(csvSchema).load(capture_adls_path)\r\n",
        "\r\n",
        "display(CheckByTPI_capture_df.limit(10))\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": true
      },
      "source": [
        "%%pyspark\r\n",
        "\r\n",
        "from pyspark.sql.functions import to_date, to_timestamp, col, date_format, current_timestamp\r\n",
        "df_final = (CheckByTPI_capture_df.withColumn('loadDate', date_format(current_timestamp(), 'MM/dd/yyyy hh:mm:ss aa'))\r\n",
        "                                 .withColumn(\"load_date\", to_timestamp(col(\"loadDate\"),\"MM/dd/yyyy hh:mm:ss aa\")).drop(\"loadDate\")\r\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      },
      "source": [
        "%%pyspark\n",
        "\n",
        "# Show Schema\n",
        "df_final.printSchema()\n",
        "\n",
        "display(df_final.limit(10))\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": true
      },
      "source": [
        "%%pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Primary storage info\n",
        "compose_account_name = 'splacceler5lmevhdeon4ym' # fill in your primary account name\n",
        "compose_container_name = 'compose' # fill in your container name\n",
        "compose_relative_path = 'SeattlePublicLibrary/CheckoutsByTitle/' # fill in your relative folder path\n",
        "\n",
        "compose_adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (compose_container_name, compose_account_name, compose_relative_path)\n",
        "print('Primary storage account path: ' + compose_adls_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": true
      },
      "source": [
        "%%pyspark\n",
        "\n",
        "compose_parquet_path = compose_adls_path + 'checkouts.parquet'\n",
        "\n",
        "print('parquet file path: ' + compose_parquet_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": true
      },
      "source": [
        "%%pyspark\n",
        "\n",
        "df_final.write.parquet(compose_parquet_path, mode = 'overwrite')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "\n",
        "-- Create database SeattlePublicLibrary only if database with same name does not exist\n",
        "CREATE DATABASE IF NOT EXISTS SeattlePublicLibrary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "\n",
        "-- Create table CheckoutsByTitlePhysicalItemsschemafinal only if table with same name does not exist\n",
        "CREATE TABLE IF NOT EXISTS SeattlePublicLibrary.checkouts_by_title\n",
        "(usage_class STRING\n",
        ",checkout_type STRING\n",
        ",material_type STRING\n",
        ",checkout_year INT\n",
        ",checkout_month INT\n",
        ",checkouts INT\n",
        ",title STRING\n",
        ",creator STRING\n",
        ",subjects STRING\n",
        ",publisher STRING\n",
        ",publication_year STRING\n",
        ",load_date TIMESTAMP\n",
        ")\n",
        "USING PARQUET OPTIONS (path 'abfss://compose@splacceler5lmevhdeon4ym.dfs.core.windows.net/SeattlePublicLibrary/CheckoutsByTitle/checkouts.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "\r\n",
        "--DROP TABLE SeattlePublicLibrary.Checkouts_By_Title"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "synapse_widget": {
      "version": "0.1"
    }
  }
}