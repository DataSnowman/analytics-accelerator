{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      },
      "source": [
        "%%pyspark\n",
        "\n",
        "df = spark.read.load('abfss://capture@splacceler5lmevhdeon4ym.dfs.core.windows.net/SeattlePublicLibrary/Integrated_Library_System__ILS__Data_Dictionary.csv', format='csv'\n",
        "## If header exists uncomment line below\n",
        ", header=True\n",
        ")\n",
        "display(df.limit(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": true
      },
      "source": [
        "%%pyspark\r\n",
        "\r\n",
        "# Show Schema\r\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": true
      },
      "source": [
        "%%pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Primary storage info\n",
        "capture_account_name = 'splacceler5lmevhdeon4ym' # fill in your primary account name\n",
        "capture_container_name = 'capture' # fill in your container name\n",
        "capture_relative_path = 'SeattlePublicLibrary/Integrated_Library_System__ILS__Data_Dictionary.csv' # fill in your relative folder path\n",
        "\n",
        "capture_adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (capture_container_name, capture_account_name, capture_relative_path)\n",
        "print('Primary storage account path: ' + capture_adls_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      },
      "source": [
        "%%pyspark\r\n",
        "\r\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType, TimestampType\r\n",
        "csvSchema = StructType([\r\n",
        "  StructField('code', StringType(), True),\r\n",
        "  StructField('description', StringType(), True),\r\n",
        "  StructField('code_type', StringType(), True), \r\n",
        "  StructField('format_group', StringType(), True),\r\n",
        "  StructField('format_subgroup', StringType(), True),\r\n",
        "  StructField('category_group', StringType(), True),\r\n",
        "  StructField('category_subgroup', StringType(), True),\r\n",
        "  StructField('age_group', StringType(), True)   \r\n",
        "])\r\n",
        "\r\n",
        "CheckByTPI_capture_df = spark.read.format('csv').option('header', 'True').schema(csvSchema).load(capture_adls_path)\r\n",
        "\r\n",
        "display(CheckByTPI_capture_df.limit(10))\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": true
      },
      "source": [
        "%%pyspark\r\n",
        "\r\n",
        "from pyspark.sql.functions import to_date, to_timestamp, col, date_format, current_timestamp\r\n",
        "df_final = (CheckByTPI_capture_df.withColumn('loadDate', date_format(current_timestamp(), 'MM/dd/yyyy hh:mm:ss aa'))\r\n",
        "                                 .withColumn(\"load_date\", to_timestamp(col(\"loadDate\"),\"MM/dd/yyyy hh:mm:ss aa\")).drop(\"loadDate\")\r\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      },
      "source": [
        "%%pyspark\n",
        "\n",
        "# Show Schema\n",
        "df_final.printSchema()\n",
        "\n",
        "display(df_final.limit(10))\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": true
      },
      "source": [
        "%%pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Primary storage info\n",
        "compose_account_name = 'splacceler5lmevhdeon4ym' # fill in your primary account name\n",
        "compose_container_name = 'compose' # fill in your container name\n",
        "compose_relative_path = 'SeattlePublicLibrary/IntegratedLibrarySystemILSDataDictionary/' # fill in your relative folder path\n",
        "\n",
        "compose_adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (compose_container_name, compose_account_name, compose_relative_path)\n",
        "print('Primary storage account path: ' + compose_adls_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": true
      },
      "source": [
        "%%pyspark\n",
        "\n",
        "compose_parquet_path = compose_adls_path + 'datadictionary.parquet'\n",
        "\n",
        "print('parquet file path: ' + compose_parquet_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": true
      },
      "source": [
        "%%pyspark\n",
        "\n",
        "df_final.write.parquet(compose_parquet_path, mode = 'overwrite')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "\n",
        "-- Create database SeattlePublicLibrary only if database with same name does not exist\n",
        "CREATE DATABASE IF NOT EXISTS SeattlePublicLibrary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "\n",
        "-- Create table CheckoutsByTitlePhysicalItemsschemafinal only if table with same name does not exist\n",
        "CREATE TABLE IF NOT EXISTS SeattlePublicLibrary.integrated_library_system_ils_data_dictionary\n",
        "(code STRING\n",
        " ,description STRING\n",
        " ,code_type STRING\n",
        " ,format_group STRING\n",
        " ,format_subgroup STRING\n",
        " ,category_group STRING\n",
        " ,category_subgroup STRING\n",
        " ,age_group STRING\n",
        " ,load_date TIMESTAMP\n",
        ")\n",
        "USING PARQUET OPTIONS (path 'abfss://compose@splacceler5lmevhdeon4ym.dfs.core.windows.net/SeattlePublicLibrary/IntegratedLibrarySystemILSDataDictionary/datadictionary.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "\r\n",
        "--DROP TABLE SeattlePublicLibrary.integrated_library_system_ils_data_dictionary"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  }
}